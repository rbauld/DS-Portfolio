{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "We start by loading the dataset, looking and some summary statistics, and the first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape is:  (284807, 31)\n",
      "Data types are:\n",
      "Time      float64\n",
      "V1        float64\n",
      "V2        float64\n",
      "V3        float64\n",
      "V4        float64\n",
      "V5        float64\n",
      "V6        float64\n",
      "V7        float64\n",
      "V8        float64\n",
      "V9        float64\n",
      "V10       float64\n",
      "V11       float64\n",
      "V12       float64\n",
      "V13       float64\n",
      "V14       float64\n",
      "V15       float64\n",
      "V16       float64\n",
      "V17       float64\n",
      "V18       float64\n",
      "V19       float64\n",
      "V20       float64\n",
      "V21       float64\n",
      "V22       float64\n",
      "V23       float64\n",
      "V24       float64\n",
      "V25       float64\n",
      "V26       float64\n",
      "V27       float64\n",
      "V28       float64\n",
      "Amount    float64\n",
      "Class       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/creditcard.csv')\n",
    "print(\"Data shape is: \", df.shape)\n",
    "print(\"Data types are:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all of our data is numeric and none of the features are named. This is anonomized data, so its not likely we will get any information about what each feature actually means. This leaves us with little in the way of feature engineering to be done. With purly numerical features on tabular data the most we can normally do is arithmetic transformations. This can lead to overfitting however, so we will avoid it here.\n",
    "\n",
    "Lets look at some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>9.481386e+04</td>\n",
       "      <td>47488.145955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54201.500000</td>\n",
       "      <td>84692.000000</td>\n",
       "      <td>139320.500000</td>\n",
       "      <td>172792.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>1.958696</td>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-0.920373</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>1.315642</td>\n",
       "      <td>2.454930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>1.651309</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-0.598550</td>\n",
       "      <td>0.065486</td>\n",
       "      <td>0.803724</td>\n",
       "      <td>22.057729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>1.516255</td>\n",
       "      <td>-48.325589</td>\n",
       "      <td>-0.890365</td>\n",
       "      <td>0.179846</td>\n",
       "      <td>1.027196</td>\n",
       "      <td>9.382558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>1.415869</td>\n",
       "      <td>-5.683171</td>\n",
       "      <td>-0.848640</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>0.743341</td>\n",
       "      <td>16.875344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>1.380247</td>\n",
       "      <td>-113.743307</td>\n",
       "      <td>-0.691597</td>\n",
       "      <td>-0.054336</td>\n",
       "      <td>0.611926</td>\n",
       "      <td>34.801666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>1.332271</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-0.768296</td>\n",
       "      <td>-0.274187</td>\n",
       "      <td>0.398565</td>\n",
       "      <td>73.301626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>1.237094</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-0.554076</td>\n",
       "      <td>0.040103</td>\n",
       "      <td>0.570436</td>\n",
       "      <td>120.589494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>1.194353</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-0.208630</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.327346</td>\n",
       "      <td>20.007208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>1.098632</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-0.643098</td>\n",
       "      <td>-0.051429</td>\n",
       "      <td>0.597139</td>\n",
       "      <td>15.594995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.768627e-15</td>\n",
       "      <td>1.088850</td>\n",
       "      <td>-24.588262</td>\n",
       "      <td>-0.535426</td>\n",
       "      <td>-0.092917</td>\n",
       "      <td>0.453923</td>\n",
       "      <td>23.745136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>9.170318e-16</td>\n",
       "      <td>1.020713</td>\n",
       "      <td>-4.797473</td>\n",
       "      <td>-0.762494</td>\n",
       "      <td>-0.032757</td>\n",
       "      <td>0.739593</td>\n",
       "      <td>12.018913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.810658e-15</td>\n",
       "      <td>0.999201</td>\n",
       "      <td>-18.683715</td>\n",
       "      <td>-0.405571</td>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.618238</td>\n",
       "      <td>7.848392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.693438e-15</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>-5.791881</td>\n",
       "      <td>-0.648539</td>\n",
       "      <td>-0.013568</td>\n",
       "      <td>0.662505</td>\n",
       "      <td>7.126883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.479045e-15</td>\n",
       "      <td>0.958596</td>\n",
       "      <td>-19.214325</td>\n",
       "      <td>-0.425574</td>\n",
       "      <td>0.050601</td>\n",
       "      <td>0.493150</td>\n",
       "      <td>10.526766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>3.482336e-15</td>\n",
       "      <td>0.915316</td>\n",
       "      <td>-4.498945</td>\n",
       "      <td>-0.582884</td>\n",
       "      <td>0.048072</td>\n",
       "      <td>0.648821</td>\n",
       "      <td>8.877742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.392007e-15</td>\n",
       "      <td>0.876253</td>\n",
       "      <td>-14.129855</td>\n",
       "      <td>-0.468037</td>\n",
       "      <td>0.066413</td>\n",
       "      <td>0.523296</td>\n",
       "      <td>17.315112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-7.528491e-16</td>\n",
       "      <td>0.849337</td>\n",
       "      <td>-25.162799</td>\n",
       "      <td>-0.483748</td>\n",
       "      <td>-0.065676</td>\n",
       "      <td>0.399675</td>\n",
       "      <td>9.253526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>4.328772e-16</td>\n",
       "      <td>0.838176</td>\n",
       "      <td>-9.498746</td>\n",
       "      <td>-0.498850</td>\n",
       "      <td>-0.003636</td>\n",
       "      <td>0.500807</td>\n",
       "      <td>5.041069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>9.049732e-16</td>\n",
       "      <td>0.814041</td>\n",
       "      <td>-7.213527</td>\n",
       "      <td>-0.456299</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.458949</td>\n",
       "      <td>5.591971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>5.085503e-16</td>\n",
       "      <td>0.770925</td>\n",
       "      <td>-54.497720</td>\n",
       "      <td>-0.211721</td>\n",
       "      <td>-0.062481</td>\n",
       "      <td>0.133041</td>\n",
       "      <td>39.420904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>0.734524</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-0.228395</td>\n",
       "      <td>-0.029450</td>\n",
       "      <td>0.186377</td>\n",
       "      <td>27.202839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>0.725702</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-0.542350</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.528554</td>\n",
       "      <td>10.503090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>0.624460</td>\n",
       "      <td>-44.807735</td>\n",
       "      <td>-0.161846</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>0.147642</td>\n",
       "      <td>22.528412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>0.605647</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-0.354586</td>\n",
       "      <td>0.040976</td>\n",
       "      <td>0.439527</td>\n",
       "      <td>4.584549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>0.521278</td>\n",
       "      <td>-10.295397</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>0.350716</td>\n",
       "      <td>7.519589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>0.482227</td>\n",
       "      <td>-2.604551</td>\n",
       "      <td>-0.326984</td>\n",
       "      <td>-0.052139</td>\n",
       "      <td>0.240952</td>\n",
       "      <td>3.517346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>0.403632</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-0.070840</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.091045</td>\n",
       "      <td>31.612198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>0.330083</td>\n",
       "      <td>-15.430084</td>\n",
       "      <td>-0.052960</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.078280</td>\n",
       "      <td>33.847808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>8.834962e+01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>25691.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>284807.0</td>\n",
       "      <td>1.727486e-03</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count          mean           std         min           25%  \\\n",
       "Time    284807.0  9.481386e+04  47488.145955    0.000000  54201.500000   \n",
       "V1      284807.0  3.919560e-15      1.958696  -56.407510     -0.920373   \n",
       "V2      284807.0  5.688174e-16      1.651309  -72.715728     -0.598550   \n",
       "V3      284807.0 -8.769071e-15      1.516255  -48.325589     -0.890365   \n",
       "V4      284807.0  2.782312e-15      1.415869   -5.683171     -0.848640   \n",
       "V5      284807.0 -1.552563e-15      1.380247 -113.743307     -0.691597   \n",
       "V6      284807.0  2.010663e-15      1.332271  -26.160506     -0.768296   \n",
       "V7      284807.0 -1.694249e-15      1.237094  -43.557242     -0.554076   \n",
       "V8      284807.0 -1.927028e-16      1.194353  -73.216718     -0.208630   \n",
       "V9      284807.0 -3.137024e-15      1.098632  -13.434066     -0.643098   \n",
       "V10     284807.0  1.768627e-15      1.088850  -24.588262     -0.535426   \n",
       "V11     284807.0  9.170318e-16      1.020713   -4.797473     -0.762494   \n",
       "V12     284807.0 -1.810658e-15      0.999201  -18.683715     -0.405571   \n",
       "V13     284807.0  1.693438e-15      0.995274   -5.791881     -0.648539   \n",
       "V14     284807.0  1.479045e-15      0.958596  -19.214325     -0.425574   \n",
       "V15     284807.0  3.482336e-15      0.915316   -4.498945     -0.582884   \n",
       "V16     284807.0  1.392007e-15      0.876253  -14.129855     -0.468037   \n",
       "V17     284807.0 -7.528491e-16      0.849337  -25.162799     -0.483748   \n",
       "V18     284807.0  4.328772e-16      0.838176   -9.498746     -0.498850   \n",
       "V19     284807.0  9.049732e-16      0.814041   -7.213527     -0.456299   \n",
       "V20     284807.0  5.085503e-16      0.770925  -54.497720     -0.211721   \n",
       "V21     284807.0  1.537294e-16      0.734524  -34.830382     -0.228395   \n",
       "V22     284807.0  7.959909e-16      0.725702  -10.933144     -0.542350   \n",
       "V23     284807.0  5.367590e-16      0.624460  -44.807735     -0.161846   \n",
       "V24     284807.0  4.458112e-15      0.605647   -2.836627     -0.354586   \n",
       "V25     284807.0  1.453003e-15      0.521278  -10.295397     -0.317145   \n",
       "V26     284807.0  1.699104e-15      0.482227   -2.604551     -0.326984   \n",
       "V27     284807.0 -3.660161e-16      0.403632  -22.565679     -0.070840   \n",
       "V28     284807.0 -1.206049e-16      0.330083  -15.430084     -0.052960   \n",
       "Amount  284807.0  8.834962e+01    250.120109    0.000000      5.600000   \n",
       "Class   284807.0  1.727486e-03      0.041527    0.000000      0.000000   \n",
       "\n",
       "                 50%            75%            max  \n",
       "Time    84692.000000  139320.500000  172792.000000  \n",
       "V1          0.018109       1.315642       2.454930  \n",
       "V2          0.065486       0.803724      22.057729  \n",
       "V3          0.179846       1.027196       9.382558  \n",
       "V4         -0.019847       0.743341      16.875344  \n",
       "V5         -0.054336       0.611926      34.801666  \n",
       "V6         -0.274187       0.398565      73.301626  \n",
       "V7          0.040103       0.570436     120.589494  \n",
       "V8          0.022358       0.327346      20.007208  \n",
       "V9         -0.051429       0.597139      15.594995  \n",
       "V10        -0.092917       0.453923      23.745136  \n",
       "V11        -0.032757       0.739593      12.018913  \n",
       "V12         0.140033       0.618238       7.848392  \n",
       "V13        -0.013568       0.662505       7.126883  \n",
       "V14         0.050601       0.493150      10.526766  \n",
       "V15         0.048072       0.648821       8.877742  \n",
       "V16         0.066413       0.523296      17.315112  \n",
       "V17        -0.065676       0.399675       9.253526  \n",
       "V18        -0.003636       0.500807       5.041069  \n",
       "V19         0.003735       0.458949       5.591971  \n",
       "V20        -0.062481       0.133041      39.420904  \n",
       "V21        -0.029450       0.186377      27.202839  \n",
       "V22         0.006782       0.528554      10.503090  \n",
       "V23        -0.011193       0.147642      22.528412  \n",
       "V24         0.040976       0.439527       4.584549  \n",
       "V25         0.016594       0.350716       7.519589  \n",
       "V26        -0.052139       0.240952       3.517346  \n",
       "V27         0.001342       0.091045      31.612198  \n",
       "V28         0.011244       0.078280      33.847808  \n",
       "Amount     22.000000      77.165000   25691.160000  \n",
       "Class       0.000000       0.000000       1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks to be somewhat normalized. All the means are quite close to zero, and the standard deviations range from 0.5 to 2. Depending on the model we choose we may need to normalize the data. A SVM model would almost certianly require further normalization to get good results. Tree based models tend not to require normalization.\n",
    "\n",
    "# Class imbalance\n",
    "Now lets look at the class balance. In fraud detection the rate of positive classes is typically very low (less than 1%). It is important we take this into account when building our model. It is very easy to build a model that correctly classifies 99% of the data if we simply predict negative/False for the entire data set. This model is not useful for fraud detection however.\n",
    "\n",
    "In these cases there are a few strategies we can try.\n",
    "\n",
    "1. Use a scoring metric that is insensitive to class imbalance. Examples of this are Area under the ROC and F1-Score\n",
    "2. Upsample or Downsample the dataset to elimate the class imbalance\n",
    "3. When training the ML model, use a weighted loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative classes: 284315\n",
      "positive classes: 492\n",
      "ratio: 0.001727485630620034\n"
     ]
    }
   ],
   "source": [
    "not_features = ['Class', 'Time']\n",
    "all_features = [x for x in df.columns if x not in not_features]\n",
    "print('negative classes:', sum(df['Class'] == 0))\n",
    "print('positive classes:', sum(df['Class'] == 1))\n",
    "print('ratio:', df['Class'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some simple feature selection. This this case we will just remove features that have very low corrilation to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,14))\n",
    "df_corr = df.corr()\n",
    "\n",
    "ax = sns.heatmap(df_corr, linewidths=.5, square=True, annot=True, fmt='.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [x for x in df_corr[df_corr['Class'].abs() > 0.05][['Class']].index if x not in not_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like some way to validate our model, so we will split the data into a train and test set. The test set will be kept until the very end to check if we are overfitting. For model tuning, we will be using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = int(len(df)*0.7)\n",
    "train_df = df.iloc[:split]\n",
    "test_df = df.iloc[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import lightgbm as lgbm\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "model = lgbm.LGBMClassifier(random_state=0, n_jobs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets see how the a default LGBM model performs. We did some feature selection, so its a good idea to check if removing those features was a good idea. Often there are non-linear relations in data and simply removing features with low corrilation can give worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean score:  0.26905332645\n",
      "score std:  0.173131260422\n"
     ]
    }
   ],
   "source": [
    "result = cross_val_score(estimator=model, X=train_df[features], y=train_df['Class'], cv=kfold, scoring='f1')\n",
    "\n",
    "print('mean score: ', np.mean(result))\n",
    "print('score std: ',np.std(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean score:  0.397089286786\n",
      "score std:  0.180945240547\n"
     ]
    }
   ],
   "source": [
    "result = cross_val_score(estimator=model, X=train_df[all_features], y=train_df['Class'], cv=kfold, scoring='f1')\n",
    "\n",
    "print('mean score: ', np.mean(result))\n",
    "print('score std: ',np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our f1 score is much better with all the features included, so we will include all of them in the remander of the training.\n",
    "\n",
    "Now lets compare a few different models. We will check if using a weighted loss function gives better results, and we will check two different boosted tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_scores(feat_list, estimator, n_reps=1):\n",
    "    result = []\n",
    "    for i in range(n_reps):\n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "        splits = kfold.split(X=train_df[feat_list], y=train_df['Class']) \n",
    "        score = cross_val_score(estimator=estimator, X=train_df[feat_list], y=train_df['Class'], cv=splits, scoring='f1')\n",
    "        result += list(score)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm weighted 0.762343259776\n",
      "xgb weighted 0.0\n",
      "lgbm default 0.305161573662\n",
      "xgb default 0.852409453922\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_list = []\n",
    "\n",
    "weight_dict = {0: train_df['Class'].mean(), 1:1/train_df['Class'].mean()}\n",
    "model_list.append(('lgbm weighted', lgbm.LGBMClassifier(class_weight=weight_dict, \n",
    "                                                        random_state=0, \n",
    "                                                        n_jobs=7)))\n",
    "model_list.append(('xgb weighted', xgb.XGBClassifier(random_state=0,\n",
    "                                                     n_jobs=7,\n",
    "                                                     scale_pos_weight=sum(train_df['Class']==1)/sum(train_df['Class']==0))))\n",
    "model_list.append(('lgbm default', lgbm.LGBMClassifier(random_state=0,\n",
    "                                                      n_jobs=7)))\n",
    "model_list.append(('xgb default', xgb.XGBClassifier(random_state=0,\n",
    "                                                   n_jobs=7)))\n",
    "\n",
    "for label, model in model_list:\n",
    "    score = calc_scores(all_features, estimator=model, n_reps=5)\n",
    "    print(label, np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our weighted LightGBM model looks much better, but interestingly enough, the default XGBoost model is performing better, so we will continue with XGBoost as our main model.\n",
    "\n",
    "An f1 score of ~0.85 is quite good all things considered, but we can try to get some improvement by doing some model tuning. Here we will do some simple tuning. There are some fairly complex and lengthy methods we can employ to tune a model, but they tend to give only marginal improvements when compared with a simple grid search across some select parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.793200878223\n",
      "0.05 0.846185834246\n",
      "0.1 0.852409453922\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for param in [0.01,0.05,0.1]:\n",
    "    model = xgb.XGBClassifier(random_state=0, learning_rate=param, n_jobs=7)\n",
    "    score = calc_scores(all_features, estimator=model, n_reps=5)\n",
    "    result.append((param, np.mean(score)))\n",
    "    print(param, np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.853910510358\n",
      "0.6 0.859727479774\n",
      "0.7 0.857970291882\n",
      "0.8 0.856966034868\n",
      "0.9 0.855173748065\n",
      "1 0.852409453922\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for param in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    model = xgb.XGBClassifier(random_state=0, learning_rate=0.1, subsample=param, n_jobs=7)\n",
    "    score = calc_scores(all_features, estimator=model, n_reps=5)\n",
    "    result.append((param, np.mean(score)))\n",
    "    print(param, np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.861995713354\n",
      "0.6 0.861012062285\n",
      "0.7 0.859574192337\n",
      "0.8 0.86078150756\n",
      "0.9 0.860119479661\n",
      "1 0.859727479774\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for param in [0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    model = xgb.XGBClassifier(random_state=0, learning_rate=0.1, subsample=0.6, colsample_bytree=param, n_jobs=7)\n",
    "    score = calc_scores(all_features, estimator=model, n_reps=5)\n",
    "    result.append((param, np.mean(score)))\n",
    "    print(param, np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gotten little improvement by model tuning, so lets stop here and train/test our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=7,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=0.6, verbosity=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(random_state=0, \n",
    "                          learning_rate=0.1, \n",
    "                          subsample=0.6, \n",
    "                          colsample_bytree=0.5, \n",
    "                          n_jobs=7)\n",
    "\n",
    "model.fit(X=train_df[all_features], y=train_df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_pred=model.predict(test_df[all_features]), y_true=test_df['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! The score on our hold out set is comparible to the score we were achiving in cross validation, which means we are not overfitting very much, and the model is generalizing very well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
